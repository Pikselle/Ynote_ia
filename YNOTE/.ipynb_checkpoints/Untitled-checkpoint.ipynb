{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3fe80a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données d'entraînement: (160, 128, 128, 1)\n",
      "Étiquettes d'entraînement: (160,)\n",
      "Données de test: (40, 128, 128, 1)\n",
      "Étiquettes de test: (40,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "\n",
    "# Chemins vers les dossiers pop et techno\n",
    "pop_dir = \"./Data/genres_original/classical/\"\n",
    "techno_dir = \"./Data/genres_original/metal/\"\n",
    "\n",
    "# Taille fixe pour les spectrogrammes (à ajuster selon vos besoins)\n",
    "input_shape = (128, 128)\n",
    "\n",
    "# Fonction pour charger les spectrogrammes à partir des fichiers audio\n",
    "def load_spectrogram(file_path, input_shape):\n",
    "    y, sr = librosa.load(file_path, sr=None)  # Charge le fichier audio\n",
    "    spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)  # Convertit en spectrogramme\n",
    "    spectrogram = librosa.power_to_db(spectrogram, ref=np.max)  # Normalise les valeurs\n",
    "    # Redimensionne le spectrogramme pour correspondre à la taille fixe\n",
    "    spectrogram = spectrogram[:input_shape[0], :input_shape[1]]\n",
    "    # Redimensionne pour avoir les dimensions attendues par le modèle\n",
    "    spectrogram = np.expand_dims(spectrogram, axis=-1)\n",
    "    return spectrogram\n",
    "\n",
    "# Fonction pour charger toutes les données d'un répertoire\n",
    "def load_data_from_directory(directory, label, input_shape):\n",
    "    data = []\n",
    "    for filename in glob.glob(os.path.join(directory, '*.wav')):\n",
    "        spectrogram = load_spectrogram(filename, input_shape)\n",
    "        data.append((spectrogram, label))\n",
    "    return data\n",
    "\n",
    "# Chargement des données pour le genre pop\n",
    "pop_data = load_data_from_directory(pop_dir, label=0, input_shape=input_shape)\n",
    "# Chargement des données pour le genre techno\n",
    "techno_data = load_data_from_directory(techno_dir, label=1, input_shape=input_shape)\n",
    "\n",
    "# Fusion des données\n",
    "all_data = pop_data + techno_data\n",
    "\n",
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "train_data, test_data = train_test_split(all_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Séparation des données d'entraînement en données et étiquettes\n",
    "train_data, train_labels = zip(*train_data)\n",
    "train_data = np.array(train_data)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Séparation des données de test en données et étiquettes\n",
    "test_data, test_labels = zip(*test_data)\n",
    "test_data = np.array(test_data)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "print(\"Données d'entraînement:\", train_data.shape)\n",
    "print(\"Étiquettes d'entraînement:\", train_labels.shape)\n",
    "print(\"Données de test:\", test_data.shape)\n",
    "print(\"Étiquettes de test:\", test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78dacd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\Maure\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Maure\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "4/4 [==============================] - 6s 472ms/step - loss: 11.8678 - accuracy: 0.4688 - val_loss: 5.6384 - val_accuracy: 0.4688\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 1s 257ms/step - loss: 2.9533 - accuracy: 0.4375 - val_loss: 0.7077 - val_accuracy: 0.4688\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 1s 258ms/step - loss: 0.7122 - accuracy: 0.5469 - val_loss: 0.6701 - val_accuracy: 0.4688\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 1s 258ms/step - loss: 0.9241 - accuracy: 0.4531 - val_loss: 0.6857 - val_accuracy: 0.5312\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 1s 254ms/step - loss: 0.6771 - accuracy: 0.5312 - val_loss: 0.6002 - val_accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 1s 249ms/step - loss: 0.5447 - accuracy: 0.8594 - val_loss: 0.5910 - val_accuracy: 0.4688\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 1s 251ms/step - loss: 0.5336 - accuracy: 0.7422 - val_loss: 0.4915 - val_accuracy: 0.7812\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 1s 251ms/step - loss: 0.4299 - accuracy: 0.7812 - val_loss: 0.3833 - val_accuracy: 0.8750\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 1s 247ms/step - loss: 0.3143 - accuracy: 0.9219 - val_loss: 0.2468 - val_accuracy: 0.9375\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 1s 239ms/step - loss: 0.1704 - accuracy: 0.9609 - val_loss: 0.2100 - val_accuracy: 0.9375\n",
      "2/2 [==============================] - 1s 39ms/step - loss: 0.1276 - accuracy: 0.9500\n",
      "Test accuracy: 0.949999988079071\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Définition des dimensions des spectrogrammes\n",
    "input_shape = train_data.shape[1:]  # Shape de chaque spectrogramme (sans le nombre d'échantillons)\n",
    "\n",
    "# Création du modèle CNN\n",
    "def create_model(input_shape):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')  # Couche de sortie avec activation sigmoid pour la classification binaire\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Création du modèle\n",
    "model = create_model(input_shape)\n",
    "\n",
    "# Compilation du modèle\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',  # Binary Crossentropy pour la classification binaire\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Entraînement du modèle\n",
    "model.fit(train_data, train_labels, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Évaluation du modèle\n",
    "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b27055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import glob\n",
    "\n",
    "classical_music_dir = \"./Data/genres_original/classical/\"\n",
    "metal_music_dir = \"./Data/genres_original/metal/\"\n",
    "\n",
    "# Charger les données musicales classiques\n",
    "classical_music_files = glob.glob(classical_music_dir +'*.wav')\n",
    "\n",
    "# Charger les données musicales metal\n",
    "metal_music_files = glob.glob(metal_music_dir+ '*.wav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e812a601",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install magenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ff942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import magenta.models.music_vae as music_vae\n",
    "\n",
    "music_vae_model = music_vae.MusicVAE(\"cat-mel_2bar_big\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e266d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metal_music(classical_music_input):\n",
    "    # Charger la musique classique en utilisant le modèle de Magenta\n",
    "    sequence = music_vae_model.improvise(classical_music_input)\n",
    "    # Convertir la séquence MIDI en audio\n",
    "    audio = sequence.to_note_sequence().to_audio()\n",
    "    return audio\n",
    "\n",
    "# Exemple d'utilisation\n",
    "classical_music_input = librosa.load(classical_music_files[0], sr=None)\n",
    "metal_music_output = generate_metal_music(classical_music_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfa5092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "# Spécifiez le chemin de destination où vous souhaitez enregistrer le fichier audio généré\n",
    "destination_path = \"./metal_music_generated.wav\"\n",
    "\n",
    "# Enregistrez le fichier audio généré\n",
    "sf.write(destination_path, metal_music_output, sample_rate)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
